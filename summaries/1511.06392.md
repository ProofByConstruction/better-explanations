# Neural Random-Access Machines

Authors: Karol Kurach, Marcin Andrychowicz, Ilya Sutskever. http://arxiv.org/abs/1511.06392

## Short Version

A controller network acts upon a set of dynamically sized fuzzy registers (which can address into a large external memory) by dispatching them through circuits via an attention mechanism.

## Problem Setting

Similar to [NTMs](1410.5401.md), [stack-RNNs](1503.01007.md), and [Neural GPUs](1511.08228.md), we want to learn solutions to algorithmic tasks that can generalize to longer sequence lengths. This is approached with NRAMs with a dynamically sized memory which can be written to and read from by fuzzy pointers which evolve in time by a circuit that the controller learns to make use of. Furthermore, the size of these fuzzy pointers (equal to the number of cells in the memory) is abstracted from the trainable controller network so that it can learn to act upon this memory regardless of its shape, allowing for generalization.

## Architecture

A Neural Random-Access Machine, NRAM, is described by a controller network (usually an LSTM), a set of R registers `r_i` of length M (R is fixed, M can be changed depending on the problem;, a set of Q prespecified (i.e. the network does not choose them, they're chosen by hand\[1\]) "gates" `m_i` which are (differentiable) functions that act on two registers, and an external memory of shape M by M (where we may think of the rows as stored values of registers). Problems are given to the network by writing them into the external memory and solutions are read out of this memory. Registers and memory rows are represented as probability distributions over their M values (over `{0, \dots, M-1}`), but are often referenced as just the scalar values.

At each step the controller network gets its previous state (if recurrent) and the probability that each `r_i` is 0 (i.e. the first column of the R by M matrix of `r_i`'s). **This is the crux that allows the controller to train on small memory sizes and generalize to larger ones** because its input size is not dependent on M. The controller then outputs gives a description for updating the registers by using the gates (see more in Training below) and a probability of stopping. At test time if the probability of stopping is high then we read out the external memory as the solution.

The gates are a hand-picked set of differentiable functions, with little other constraint. In the paper they used gates such as m(a,b) = 1, m(a,b) = a+1 (mod M) (which in the setting where a is a register representing probabilities over M values is simply a coordinate right-shift (hence differentiable)), `m(a,b) = \[a=b\]` (see Differentiability of Gates below for more detail on implementation), and a couple special gates for READ and WRITE. The READ gate returns a weighted sum from the memory where each row is weighted by the corresponding entry of the first argument register (the second argument is ignored). The `WRITE(r, r')` gate similarly uses the first argument to give a weighted index `(w_1, \dots, w_M)` into the memory and updates row i by `(1-w_i)M_i + w_i r'`.

## Training

All of the trainable variables live in the controller which outputs 1) attention distributions `a_i, b_i \in R^{R+i-1}` for `i \in {1,\dots,Q}` that fuzzily choose the two inputs to the i-th gate, 2) attention distributions `c_i` which, given previous registers and the outputs from the gates (ie of size R+Q), choose what to write in register i for the next time step\[2\], and 3) a stopping probability which allows the network to claim that the current contents of the memory bank contain the solution.

Since we output a probability of stopping at each step, we can compute a weighted loss where the loss of a given step (after unrolling however far we please) is weighted by the probability we didn't stop at any previous step and stopped at the last one. We sum these losses over all steps to get the final loss. Each step's loss is the negative log-likelihood of producing the correct output given the distribution encoded by the memory bank. (Note that this constrains the output to be smaller than the memory so we must have a bound on the output size before runtime.)

It isn't detailed in the paper, but presumably `a_i^t = \sigma(A_i h_t)` (or tanh) and we learn a set of 2Q matrices (since it seems a_i is differently sized for each i) which compute `a_i, b_i`, and R matrices to output `c_i` (for each of the R output registers) given the hidden state of the LSTM. These details could probably be done in a number of ways (and the controller network designed appropriately). I've not found an implementation online yet.

## Notes

\[1\] The Neural Programmer-Interpreter in contrast is a model that chooses which functions it uses over time (but these choices are trained with full execution traces). I'm curious what hybrid approaches might exist between these two around this particular mechanism. (Eg if the controller attended over the gates and computed a weighted sum over all of their outputs presumably it could learn how to pick which ones were useful, but this is likely very hard for it to learn well since the signal is so noisy.)

\[2\] This attention mechanism is very similar to what's happening in [stack-RNNs](1503.01007.md) (they take insight from the same [source](http://arxiv.org/abs/1409.0473))

### Computation of `r_i`

Equation (3) was a bit confusing for me because it's written as if `r_i` is a scalar. Taking the r's and o's to be of length M, the `(r_1, \dots, r_R, o_1, \dots, o_Q)^T` is a matrix with height M and width R+Q. Multiplying by softmax over `c_i` (of length R+Q) gives a vector of M values, the fuzzy distribution for `r_i`'s updated value.

### The differentiability of gates `{m_i}`

Upon first reading it wasn't clear to me how the gate representing a=b would allow gradients to pass through it. Eq (2) in the paper is the key, and essentially this gate is computed via the dot product of the two registers, `[1-r.r', r.r']` (followed by 0's up to length M) since the probability that they are equal is the sum over each value they can hold that they are each that value. Similar arithmetic can be used to calculate a<b in a differentiable fashion (these two are then additive in their 1 value to give <=).