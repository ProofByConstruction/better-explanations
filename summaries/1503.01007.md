# Inferring Algorithmic Patterns with Stack-Augmented Recurrent Nets

Authors: Armand Joulin, Tomas Mikolov. http://arxiv.org/abs/1503.01007

## Short Version

A minimal RNN (no LSTM or GRU gating mechanisms) is extended with an external stack that it can write to of which the top k elements are used in the recurrent connection. This stack-RNN is shown to learn simple algorithmic tasks.

## Problem Setting

In sequence to sequence modeling it's often difficult for things even as powerful as LSTMs to capture long-term sequential structure. This is in part because the recurrent connections are limited in size and lossy over time. Admittedly, LSTMs/GRUs help alleviate this with gating mechanisms that allow longer persistence of memory and things like counting, but they seem an indirect solution to such a problem.

However, instead of relying on a fixed-size, fully rewritable memory (like LSTMs), this approach considers a structured memory with accessor patterns (a stack with PUSH and POP) that can grow its storage over time.

## Architecture

The initial formulation of the network is quite short, though there are extensions to different data structures (lists), more actions on the data structure (e.g. NO-OP on a stack), and multiple stacks.

The main components are a hidden layer, whose activation at time t will be denoted h_t, and a stack whose i'th element at time t is s_t\[i\]. The crux of the entire mechanism is the fact that this memory is **continuous and differentiable.** Given this it can be trained by simple SGD.

For the memory to be continuous, we define its evolution as a smooth modification by probabilities of PUSH and POP. These probabilities are computed by softmax over `A h_t = a_t`, where A is a learnable "action matrix" (of size #actions by size of h_t (= m)). The topmost element of the stack is then set to be the weighted sum `s_t[0] = a_t[PUSH] \sigma(D h_t) + a_t[POP] s_{t-1}[1]` where D is a 1xm matrix that picks a number to store given a current hidden state, and `s_{t-1}[1]` was the previous  element under the top (i.e. if we POP, it would move into the top).  This change propagates all the way down the stack, but instead of `\sigma(D h_t)` we use the element above (eg `s_{t-1}[0]` when computing `s_{t}[1]`).

The rest is a simple update of the hidden state, `h_t = \sigma(U x_t + R h_{t-1} + P s_{t-1}[0:k])`, where U is an embedding matrix (from the input space to the dimension of h_t = m), R is a recurrent connection matrix (of size m by m), and P is a memory accessor matrix which takes the first k elements of s_t. (And sigma is componentwise sigmoid.)

## Findings

 - Tasks such as modeling simple grammars (eg `a^n b^m c^{n+m}`, a counting grammar) memorization (copying?), and binary addition all showed better performance with a stack than an LSTM.
 - In binary addition the stacks were inspected and shown to perform various functions (eg holding a carry digit).
 - Language modeling was more difficult for a stack-RNN, achieving worse perplexity than LSTM.

It seems that for some algorithmic tasks this type of structured memory approach could be helpful, but on complex natural phenomena like natural language it still has a ways to go. Of course, stack augmentation could easily be added to many recurrent models (I think this might make reasonable sense at deeper layers where understanding is more symbolic).

## Questions

 - As a function of length of examples seen during training, how well was the stack-RNN able to generalize to longer sequences?
 - What would happen if we used multiple hidden layers? How would memory be used at different layers?
 - What other types of memory structures can we extend to? What does a smoothly evolving tree memory look like?
 - The reason for the use of the sigmoid in the memory update (`\sigma(D h_t)`) isn't particularly obvious to me--how would something like a relu behave instead?

## Notes

 - At test time, it was useful to round a_t, forcing the action to be a straight PUSH or POP. For generalization to longer sequences this makes some intuitive sense as the imprecision of smooth actions could compound and degrade the memory signal. It is interesting that this works, as it signals that the probabilistic memory controller would perhaps like to be discrete (but of course we couldn't train such a controller with SGD).
