# Pointer Networks

Authors: Oriol Vinyals, Meire Fortunato, Navdeep Jaitly. http://arxiv.org/abs/1506.03134

## Short Version

Pointer Networks use an attention mechanism which picks one of the input sequence entries at each step of output, thus allowing its outputs to scale in vocabulary proportional to the input length. This is shown to learn approximate solutions to problems such as the traveling salesman, finding convex hulls, and Delaunay triangulation.

## Problem Setting

There are many sequential problems whose solutions are directly related to their inputs (eg traveling salesman), which can't be solved by other sequence-sequence approaches because of the direct relationship between their solutions and the input symbols (in particular, the authors claim that the matching between outputs and inputs degrades proportionally to sequence length).

Pointer networks approach these types of problems by computing their outputs via a dynamically sized attention over all previous input encodings.

## Architecture

There are two basic pieces in a pointer net: 1. two (vanilla)  LSTMs (an encoder that converts the input sequence x_j into hidden activations e_j, and a decoder that converts output sequence C_i to d_i) and 2. an attentional mechanism which consists of a vector v and two matrices (W_1, W_2) that returns an softmax over the input sequence for each step of output.

The **attention mechanism** [originally published here](http://arxiv.org/abs/1409.0473) is the crux that allows each step in the output to select one of the inputs. (Without it, we cannot scale our output selection with the input length.) It does this by referring to the encoded representations of each step in the input (the e_j's from above) and converting them into probabilities given the current decoded state (d_i) by computing the softmax of the scalars `u_j^i` over all j's where `u_j^i = v^T * tanh(W_1 * e_j + W_2 * d_i)`.

## Intuition

Why this might work wasn't immediately obvious to me. The fact that the e_j's and d_i's are capturing temporal dynamics (via the LSTMs that generate them) means that the attentional mechanism is able to select based on the combination of where the decoder LSTM thinks we are right now, and where the encoder thought we were at each step in the input. It would be interesting to inspect the features that are being stored in the e_j's

## Training

The paper doesn't discuss training explicitly, so I had a few questions about it. First note that in order to train at all we can define the loss at an output step as the (negative log) probability of the desired element under the softmax.

However we'll only train given an already-correct output sequence--i.e. there's no correct answer if we've gone off the solution path. I wonder if this might be particularly harmful for algorithmic tasks when the network picks an incorrect output early on, whereas with language modeling--where this attentional technique was introduced--it might be temporarily strange but can quickly correct the grammatical structure. (Alternatively, one could imagine also teaching a pointer network to correct itself by trying to pick the best solution given the incorrect solution it has picked so far--this requires an ordering on solution quality beyond a binary correct/incorrect label, but could work for eg traveling salesman.)

For the training schedule do we run through the entire training example output and at each step both update v, W_1, W_2, as well as propagate gradients through the LSTMs? If this is the case, I wonder how coupled the updates are, versus a schedule that does something more like sample an example, compute the encoded sequence, the first k correct pointers, and the decoded sequence, and then train only on the next step. (This would require more input examples proportional to their length and much more time given the encoding/decoding steps, but would provide entirely independent updates, something which can be important for training recurrent nets in RL settings, but which has unknown value here.)

## Notes

 - They tried curriculum learning but found it didn't have a noticeable effect, so just sample uniformly from all training examples. I find this particularly strange given that many other neural algorithm approaches require curricula. Is this a particular facet of the model? Is it learning all lengths independently (and could we train it on just things of length n)? Given this it's also surprising to me that it generalizes on the convex hull problem.
 - At first I thought that the computation required at each step is fairly large since we need to recompute the attentional softmax given the current decoder value. However at runtime after encoding the input we can cache W_1\*e_j for all j, so we're not performing lots of extra matrix multiplies. We still have to perform a dot product per input, of length equal to the hidden dimension, so that gives H\*n (~ 512n in their experiments) multiplies and adds per step of inference, which is small compared to the decoder LSTM (unless the sequence is very long, at which point the embedding size would likely need to increase anyway, making the LSTM even more costly).

## Other Problems to Test

Testing this approach on sorting (eg binary numbers) seems natural and I'm curious how it would fare (especially with respect to generalization).