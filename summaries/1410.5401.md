# Neural Turing Machines

Authors: Alex Graves, Greg Wayne, Ivo Danihelka. http://arxiv.org/abs/1410.5401

## Short Version

An LSTM network is augmented with an external memory that can be interacted with via a number of read and write heads in a structured way and learns to solve various algorithmic tasks.

## Problem Setting

Taken from the summary on [Stack RNNs](1503.01007.md):

In sequence to sequence modeling it's often difficult for things even as powerful as LSTMs to capture long-term sequential structure. This is in part because the recurrent connections are limited in size and lossy over time. Admittedly, LSTMs/GRUs help alleviate this with gating mechanisms that allow longer persistence of memory and things like counting, but they seem an indirect solution to such a problem.

NTMs address this by using a structured memory with an attention mechanism that selectively acts upon it via read and write heads. They're shown to learn to solve algorithmic tasks much faster than LSTMs and generalize better to longer sequences.

## Architecture

There are two main components to an NTM: a neural network controller (LSTM empirically gives better performance than feedforward) responsible for both generating output and attending to memories (they draw an analogy to a CPU), and an external memory store consisting of a memory bank, read heads, and write heads, which structures the access of the memory given the attention mechanism of the controller (analogous to RAM).

### Memory Bank
The memory bank is a matrix (size N, number of memories, by M, size of each memory) along with read/write heads responsible for returning and writing values respectively. They do this in a continuous manner such that everything is differentiable and thus trainable with SGD (familiarity with [Stack RNNs](1503.01007.md) makes this easier to understand).

Given a attentional distribution at time t `w_t^i, i \in [1,N]` (summing to 1) over the memories, the read heads return a sum over the rows of M weighted by the corresponding `w_t^i` for each row i. The write heads are staged by an erase and then an add, each driven by a vector (per head) `e_t` and `a_t` respectively.\[1\] Given these vectors, the erase multiplies each row of the memory matrix componentwise with `1-(e_t w_t^i)`, thus erasing things where `e_t` is high and it's being attended to strongly. The write is much the same, adding on an add vector to each row, weighted by the attention to that row, ie `a_t w_t^i` is added to row i.\[2\]

### Attention

The attention mechanism generates a set of weights per head, `w_t^i, i \in [1,N]` where N is the number of memories to attend over. The way this attention is computed has several steps, and depends upon the output of the controller network in ways which are left out in the paper (see also note \[1\]). The mechanisms for generating these weights are motivated by the desire to be able to 1) look up related content, 2) find block-related items to some content, and 3) iterate through memory by shifting. The two mechanisms at play are called content lookup and address lookup.

The steps in calculating the attentional weights are as follows: 1) calculate focus weights based on similarity of content to a target (so we can select a particular area of the memory related to where we are in the problem), 2) interpolate between those and the weights of the previous time step (so attention can exist over a longer time scale), 3) smoothly (for differentiabilty's sake) rotationally shift the weights (for location based addressing), 4) sharpen the final weights (to prevent blurring over longer time scales)

Content lookup uses a similar mechanism to that in [Pointer Networks](1506.03134.md). A key `k_t` and key strength `\beta_t` (likely both produced by fully connected activations off the output of the controller) are used to calculate and then scale a similarity to each row in the memory matrix. These scaled similarities are softmaxed, and so we get a vector of attentional weights `w_{t, c}`. Cosine similarity is used here, but anything differentiable would do.

After content lookup, we interpolate between `w_{t, c}` and `w_{t-1}`.

Next we calculate a distribution over shifts of the weights by different integer amounts and then use a circular convolution to "probabilistically shift" the weights. This mechanism allows for simple iteration.

Finally we sharpen by something that looks like a softmax but raises each of the weights to a power of `\gamma_t` and normalizes. Without this sharpening we could expect memories over long sequences to blur and become unusable. This bears resemblance to [Pointer Networks](1506.03134.md) using discretized actions at test time for stability.

## Notes

\[1\] It is not addressed in the paper, but from the inspection of [code here](https://github.com/carpedm20/NTM-tensorflow/blob/master/ntm_cell.py) it appears `e_t, a_t` are derived by fully connected sigmoid/tanh activations off of the previous output of the controller. We might expect similar types of connections and activations to produce the parameters k, beta, g, s, and gamma all at time t, but this decision seems to be left to the reader.

\[2\] Which order the write heads erase in doesn't matter because the erasures are commutative, similarly for adds. We should not add from some write heads and then erase from others, however.
